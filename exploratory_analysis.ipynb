{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Michelin Star Restaurant Guide Dashboard"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dataset Attributes\n",
    "\n",
    "- **Name**: The name of the Michelin-starred restaurant.\n",
    "- **Address**: The full street address of the restaurant.\n",
    "- **Location**: The city and country where the restaurant is located.\n",
    "- **Price**: Price range indicator, using $ symbols (e.g. $$$$ for very expensive).\n",
    "- **Cuisine**: The type or style of cuisine served at the restaurant.\n",
    "- **Longitude**: The geographic longitude coordinate of the restaurant's location.\n",
    "- **Latitude**: The geographic latitude coordinate of the restaurant's location.\n",
    "- **PhoneNumber**: The contact phone number for the restaurant.\n",
    "- **Url**: The URL of the restaurant's page on the official Michelin Guide website.\n",
    "- **WebsiteUrl**: The URL of the restaurant's own official website.\n",
    "- **Award**: The Michelin star rating awarded to the restaurant (e.g. \"3 Stars\").\n",
    "- **GreenStar**: A binary indicator (0 or 1) of whether the restaurant has received a Michelin Green Star for sustainability.\n",
    "- **FacilitiesAndServices**: A list of amenities and services offered by the restaurant.\n",
    "- **Description**: A brief description of the restaurant, often including details about the chef and cuisine."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dependency"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %pip install -r .\\requirements.txt\n",
    "# %pip install -q pandas plotly dash dash-bootstrap-components pyarrow python-dotenv\n",
    "# %pip freeze > requirements.txt # WARNING!! run this only on a linux distro or wsl"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import pyarrow as pa\n",
    "from pandas import DataFrame\n",
    "from pandas._typing import ArrayLike\n",
    "\n",
    "\n",
    "from IPython.core.interactiveshell import InteractiveShell\n",
    "\n",
    "InteractiveShell.ast_node_interactivity = \"all\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"data/michelin_by_Jerry_Ng.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data cleaning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Missing values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "_ = df.isna().sum()\n",
    "_[_ > 0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Price column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Cell generated by Data Wrangler.\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "def clean_data(df):\n",
    "    # Replace missing values with the most common value of each column in: 'Price'\n",
    "    df = df.fillna({\"Price\": df[\"Price\"].mode()[0]})\n",
    "    # Created column 'Standardized_Price' from formula\n",
    "    df[\"Standardized_Price\"] = df[\"Price\"].apply(len)\n",
    "    return df\n",
    "\n",
    "\n",
    "df_clean_1 = clean_data(df.copy())\n",
    "df_clean_1.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Location columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Cell generated by Data Wrangler.\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "def clean_data(df_clean: DataFrame):\n",
    "    # Split text using string ',' in column: 'Location'\n",
    "    loc_0 = df_clean.columns.get_loc(\"Location\")\n",
    "    df_clean_split = (\n",
    "        df_clean[\"Location\"].str.split(pat=\",\", expand=True).add_prefix(\"Location_\")\n",
    "    )\n",
    "    df_clean = pd.concat(\n",
    "        [df_clean.iloc[:, :loc_0], df_clean_split, df_clean.iloc[:, loc_0:]], axis=1\n",
    "    )\n",
    "    # Rename column 'Location_0' to 'Location_city'\n",
    "    df_clean = df_clean.rename(columns={\"Location_0\": \"Location_city\"})\n",
    "    # Rename column 'Location_1' to 'Location_country'\n",
    "    df_clean = df_clean.rename(columns={\"Location_1\": \"Location_country\"})\n",
    "    # Fill missing country values with this dict\n",
    "    city_country_map = {\n",
    "        \"Singapore\": \"Singapore\",\n",
    "        \"Hong Kong\": \"China\",\n",
    "        \"Macau\": \"China\",\n",
    "        \"Dubai\": \"United Arab Emirates\",\n",
    "        \"Luxembourg\": \"Luxembourg\",\n",
    "        \"Abu Dhabi\": \"United Arab Emirates\",\n",
    "    }\n",
    "    df_clean[\"Location_country\"] = df_clean[\"Location_country\"].fillna(\n",
    "        df_clean[\"Location_city\"].map(city_country_map)\n",
    "    )\n",
    "    return df_clean\n",
    "\n",
    "\n",
    "df_clean_1 = clean_data(df_clean_1.copy())\n",
    "df_clean_1.head()\n",
    "\n",
    "\n",
    "# def select_unique_location_city_where_location_country_is_missing(\n",
    "#     df_clean_1: DataFrame,\n",
    "# ) -> ArrayLike:\n",
    "#     # Filter rows based on column: 'Location_country'\n",
    "#     df_clean_1 = df_clean_1[df_clean_1[\"Location_country\"].isna()]\n",
    "#     return df_clean_1[\"Location_city\"].unique()\n",
    "\n",
    "\n",
    "# pd.DataFrame(\n",
    "#     {\n",
    "#         \"City\": select_unique_location_city_where_location_country_is_missing(\n",
    "#             df_clean_1.copy()\n",
    "#         )\n",
    "#     }\n",
    "# )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### FacilitiesAndServices columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.set_option(\"display.max_columns\", None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "unique_facilities = set()\n",
    "\n",
    "\n",
    "def clean_data(df_clean_1: DataFrame):\n",
    "    df_clean_1[\"FacilitiesAndServices\"] = df_clean_1[\"FacilitiesAndServices\"].fillna(\"\")\n",
    "    # Split text using string ',' in column: 'Cuisine'\n",
    "    loc_0 = df_clean_1.columns.get_loc(\"FacilitiesAndServices\")\n",
    "    df_clean_1_split = df_clean_1[\"FacilitiesAndServices\"].str.split(\n",
    "        pat=\",\", expand=True\n",
    "    )\n",
    "    # Remove leading and trailing whitespace in split columns\n",
    "    df_clean_1_split.apply(lambda x: x.str.strip())\n",
    "    # Extract unique facilities\n",
    "    unique_facilities = set(df_clean_1_split.values.ravel())\n",
    "    unique_facilities.discard(\"\")\n",
    "    unique_facilities.discard(None)\n",
    "    # Create new columns for each unique facility or service\n",
    "    facility_cols = {\n",
    "        f\"FacilitiesAndServices_{facility}\": df_clean_1[\"FacilitiesAndServices\"]\n",
    "        .str.contains(facility, regex=False)\n",
    "        .astype(int)\n",
    "        for facility in unique_facilities\n",
    "    }\n",
    "    # Combine all DataFrames at once (prevents memory defragmentation caused by iteratively adding or removing columns)\n",
    "    result = pd.concat(\n",
    "        [\n",
    "            df_clean_1.iloc[:, :loc_0],\n",
    "            pd.DataFrame(facility_cols),\n",
    "            df_clean_1.iloc[:, loc_0:],\n",
    "        ],\n",
    "        axis=1,\n",
    "    )\n",
    "    return result\n",
    "\n",
    "\n",
    "df_clean_2 = clean_data(df_clean_1.copy())\n",
    "df_clean_2.head()\n",
    "\n",
    "unique_facilities\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cuisine columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "unique_cuisines = set()\n",
    "\n",
    "\n",
    "def clean_data(df_clean_2: DataFrame):\n",
    "    df_clean_2[\"Cuisine\"] = df_clean_2[\"Cuisine\"].fillna(\"\")\n",
    "    # Split text using string ',' in column: 'Cuisine'\n",
    "    loc_0 = df_clean_2.columns.get_loc(\"Cuisine\")\n",
    "    df_clean_2_split = (\n",
    "        df_clean_2[\"Cuisine\"].str.split(pat=\",\", expand=True).add_prefix(\"Cuisine_\")\n",
    "    )\n",
    "    # Remove leading and trailing whitespace in split columns\n",
    "    df_clean_2_split = df_clean_2_split.apply(lambda x: x.str.strip())\n",
    "    # Extract unique cuisines\n",
    "    unique_cuisines = set(df_clean_2_split.values.ravel())\n",
    "    unique_cuisines.discard(None)\n",
    "    unique_cuisines.discard(\"\")\n",
    "    # Create new columns for each unique cuisine\n",
    "    cuisine_columns = {\n",
    "        f\"Cuisine_{cuisine}\": df_clean_2[\"Cuisine\"]\n",
    "        .str.contains(cuisine, regex=False)\n",
    "        .astype(int)\n",
    "        for cuisine in unique_cuisines\n",
    "    }\n",
    "    # Combine all DataFrames at once\n",
    "    result = pd.concat(\n",
    "        [\n",
    "            df_clean_2.iloc[:, :loc_0],\n",
    "            pd.DataFrame(cuisine_columns),\n",
    "            df_clean_2.iloc[:, loc_0:],\n",
    "        ],\n",
    "        axis=1,\n",
    "    )\n",
    "    return result\n",
    "\n",
    "\n",
    "df_clean_3 = clean_data(df_clean_2.copy())\n",
    "df_clean_3.head()\n",
    "\n",
    "# unique_cuisines"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python",
   "pygments_lexer": "ipython3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
